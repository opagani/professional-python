{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Pytest\n",
    "\n",
    "- Easy test creation\n",
    "- Test runner\n",
    "- Test selection\n",
    "- Test parametrization\n",
    "- Test fixtures\n",
    "- Plugins\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "uv add --dev pytest\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Running tests\n",
    "\n",
    "```bash\n",
    "uv run pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Creation\n",
    "\n",
    "4 steps of a test:\n",
    "\n",
    "1. Setup\n",
    "2. Exercise\n",
    "3. Verify\n",
    "4. Teardown\n",
    "\n",
    "In pytest these steps are usually done with:\n",
    "\n",
    "1. Setup: Fixtures or setup methods\n",
    "2. Exercise: Call the function or method to be tested\n",
    "3. Verify: Use assert statements\n",
    "4. Teardown: Fixtures or teardown methods\n",
    "\n",
    "## Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_add():\n",
    "    assert add(1, 2) == 3\n",
    "    assert add(2, 3) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m_________________________________________ test_that_fails __________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_that_fails\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m add(\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m) == \u001b[94m3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m add(\u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == \u001b[94m6\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 5 == 6\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 5 = add(2, 3)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/2926093103.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1mtest_that_fails\u001b[0m - assert 5 == 6\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_that_fails():\n",
    "    assert add(1, 2) == 3\n",
    "    assert add(2, 3) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Layout\n",
    "\n",
    "Tests are usually placed in a `tests` directory. Generally it does not need to be in the same directory as the code being tested. But the code that is being tested should be importable from the test directory. (This is why I like to do an editable install of the package I am testing.)\n",
    "\n",
    "add `tests/test_basic.py`:\n",
    "\n",
    "```python\n",
    "import sk_stepwise as sw\n",
    "\n",
    "\n",
    "def test_initialization():\n",
    "    model = None\n",
    "    rounds = []\n",
    "    optimizer = sw.StepwiseHyperoptOptimizer(model, rounds)\n",
    "    assert optimizer is not None\n",
    "```\n",
    "\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "uv run pytest\n",
    "```\n",
    "\n",
    "The output should be:\n",
    "\n",
    "```bash\n",
    "% uv run pytest\n",
    "===================== test session starts ===============================================\n",
    "platform darwin -- Python 3.12.5, pytest-8.3.3, pluggy-1.5.0\n",
    "rootdir: /private/tmp/sk-stepwise\n",
    "configfile: pyproject.toml\n",
    "collected 1 item\n",
    "\n",
    "tests/test_basic.py .                                                                                      [100%]\n",
    "\n",
    "================= warnings summary ================================================\n",
    ".venv/lib/python3.12/site-packages/hyperopt/atpe.py:19\n",
    "  /private/tmp/sk-stepwise/.venv/lib/python3.12/site-packages/hyperopt/atpe.py:19: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
    "    import pkg_resources\n",
    "\n",
    "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
    "============= 1 passed, 1 warning in 0.66s ==========================================\n",
    "matt@Matts-MacBook-Pro-4 sk-stepwise % cat tests/test_basic.py\n",
    "```\n",
    "\n",
    "## Test Output\n",
    "\n",
    "- `.` - test passed\n",
    "- `F` - test failed\n",
    "- `E` - test had an exception during fixture setup or teardown\n",
    "- `s` - test was skipped\n",
    "- `x` - expected failure\n",
    "- `X` - unexpected success (should have failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mE\u001b[0m\u001b[33ms\u001b[0m\u001b[33mx\u001b[0m\u001b[33mX\u001b[0m\u001b[31m                                                                                        [100%]\u001b[0m\n",
      "============================================== ERRORS ==============================================\n",
      "\u001b[31m\u001b[1m_____________________________________ ERROR at setup of test_E _____________________________________\u001b[0m\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.fixture\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfail_fixture\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/3662948318.py\u001b[0m:12: NotImplementedError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1mtest_E\u001b[0m - NotImplementedError\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "# the -qq is for quiet mode, which suppresses the output of the tests\n",
    " \n",
    "import os\n",
    "import pytest\n",
    "\n",
    "\n",
    "def test_period():\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.fixture\n",
    "def fail_fixture():\n",
    "    raise NotImplementedError\n",
    "\n",
    "def test_E(fail_fixture):\n",
    "    assert 1 == 1\n",
    "\n",
    "def test_skip():\n",
    "    if os.name == 'posix':\n",
    "        pytest.skip('skipping this test on posix')\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_x():\n",
    "    # assuming 3.14 adds the method .fancy_split() to the str class\n",
    "    assert '1,2-4'.fancy_split() == ['1', '2', '3', '4']\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_X():\n",
    "    assert ' 1'.lstrip() == '1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Different Outputs\n",
    "\n",
    "```python\n",
    "import sk_stepwise as sw\n",
    "import pytest\n",
    "\n",
    "\n",
    "def test_initialization():\n",
    "    model = None\n",
    "    rounds = []\n",
    "    optimizer = sw.StepwiseHyperoptOptimizer(model, rounds)\n",
    "    assert optimizer is not None\n",
    "\n",
    "def test_that_fails():\n",
    "    assert 'matt' == 'fred'\n",
    "\n",
    "@pytest.fixture\n",
    "def one():\n",
    "    return 1/0\n",
    "\n",
    "def test_with_exception(one):\n",
    "    assert one == 1\n",
    "\n",
    "@pytest.mark.xfail(raises=TypeError)\n",
    "def test_logistic():\n",
    "    from sklearn import linear_model\n",
    "    model = linear_model.LinearRegression()\n",
    "    rounds = []\n",
    "    opt = sw.StepwiseHyperoptOptimizer(model, rounds)\n",
    "    X = [[0,1], [0,2]]\n",
    "    y = [1, 0]\n",
    "    opt.fit(X, y)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The assert Statement\n",
    "\n",
    "Pytest rewrote the assert statement to give more information when it fails. It works with different types to give relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform darwin -- Python 3.10.14, pytest-7.2.0, pluggy-1.0.0 -- /Users/matt/.envs/menv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/Users/matt/Dropbox/work/courses/ms-courses/professionalpython/03-Testing/.hypothesis/examples')\n",
      "rootdir: /Users/matt/Dropbox/work/courses/ms-courses/professionalpython/03-Testing\n",
      "plugins: dash-2.11.1, timeout-2.1.0, pytest_check_links-0.8.0, cov-4.0.0, hypothesis-6.81.2, console-scripts-1.3.1, anyio-3.6.2, typeguard-4.0.0, mock-3.14.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items\n",
      "\n",
      "t_0fd94c1a4d2a48488d17128f29d4c46f.py::test_number \u001b[32mPASSED\u001b[0m\u001b[32m                                    [ 20%]\u001b[0m\n",
      "t_0fd94c1a4d2a48488d17128f29d4c46f.py::test_string \u001b[31mFAILED\u001b[0m\u001b[31m                                    [ 40%]\u001b[0m\n",
      "t_0fd94c1a4d2a48488d17128f29d4c46f.py::test_string_in \u001b[31mFAILED\u001b[0m\u001b[31m                                 [ 60%]\u001b[0m\n",
      "t_0fd94c1a4d2a48488d17128f29d4c46f.py::test_list_in \u001b[31mFAILED\u001b[0m\u001b[31m                                   [ 80%]\u001b[0m\n",
      "t_0fd94c1a4d2a48488d17128f29d4c46f.py::test_raise \u001b[32mPASSED\u001b[0m\u001b[31m                                     [100%]\u001b[0m\n",
      "\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m___________________________________________ test_string ____________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_string\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mThe cold brown fox ate a bird\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33mThe jumping brown fox ate a bird\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'The cold bro...ox ate a bird' == 'The jumping ...ox ate a bird'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         - The jumping brown fox ate a bird\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?     ^^^^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + The cold brown fox ate a bird\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?     ^^^^\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/782513774.py\u001b[0m:5: AssertionError\n",
      "\u001b[31m\u001b[1m__________________________________________ test_string_in __________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_string_in\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[95min\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mmatt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'b' in 'matt'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/782513774.py\u001b[0m:8: AssertionError\n",
      "\u001b[31m\u001b[1m___________________________________________ test_list_in ___________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_list_in\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94m5_000\u001b[39;49;00m \u001b[95min\u001b[39;49;00m \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[94m1000\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 5000 in [0, 1, 2, 3, 4, 5, ...]\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where [0, 1, 2, 3, 4, 5, ...] = list(range(0, 1000))\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where range(0, 1000) = range(1000)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/782513774.py\u001b[0m:11: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1mtest_string\u001b[0m - AssertionError: assert 'The cold bro...ox ate a bird' == 'The jumping ...ox ate a bird'\n",
      "\u001b[31mFAILED\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1mtest_string_in\u001b[0m - AssertionError: assert 'b' in 'matt'\n",
      "\u001b[31mFAILED\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1mtest_list_in\u001b[0m - assert 5000 in [0, 1, 2, 3, 4, 5, ...]\n",
      "\u001b[31m=================================== \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[31m ====================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_number():\n",
    "    assert 1 == 1\n",
    "\n",
    "def test_string():\n",
    "    assert 'The cold brown fox ate a bird' == 'The jumping brown fox ate a bird'\n",
    "\n",
    "def test_string_in():\n",
    "    assert 'b' in 'matt'\n",
    "\n",
    "def test_list_in():\n",
    "    assert 5_000 in list(range(1000))\n",
    "\n",
    "def test_raise():\n",
    "    # no assertion in this one\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        1 / 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytest.raises??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _pytest import python_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_api.RaisesContext??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m___________________________________________ test_number ____________________________________________\u001b[0m\n",
      "\n",
      "sales =    sales       date\n",
      "0      1 2020-01-01\n",
      "1      2 2020-01-02\n",
      "2      3 2020-01-03\n",
      "3      1 2020-01-04\n",
      "4      2 2020-01-05\n",
      "5      3 2020-01-06\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_number\u001b[39;49;00m(sales):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(sales.index, pd.DatetimeIndex), \u001b[33m'\u001b[39;49;00m\u001b[33mShould be a timeseries index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Should be a timeseries index\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(RangeIndex(start=0, stop=6, step=1), <class 'pandas.core.indexes.datetimes.DatetimeIndex'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where RangeIndex(start=0, stop=6, step=1) =    sales       date\\n0      1 2020-01-01\\n1      2 2020-01-02\\n2      3 2020-01-03\\n3      1 2020-01-04\\n4      2 2020-01-05\\n5      3 2020-01-06.index\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'pandas.core.indexes.datetimes.DatetimeIndex'> = pd.DatetimeIndex\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/4258747586.py\u001b[0m:10: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1mtest_number\u001b[0m - AssertionError: Should be a timeseries index\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "# Can provide addtional text upon failure\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.fixture\n",
    "def sales():\n",
    "    return pd.DataFrame({'sales': [1, 2, 3, 1, 2, 3],\n",
    "                            'date': pd.date_range('2020-01-01', periods=6)})\n",
    "\n",
    "def test_number(sales):\n",
    "    assert isinstance(sales.index, pd.DatetimeIndex), 'Should be a timeseries index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Runner\n",
    "\n",
    "`pytest` assumes that the code you are testing is installed. It does not add the current directory to the path. So you need to install the package you are testing. You can do this with `pip install -e .` or `uv run pip install -e .`. If you run `python -m pytest` it will add the current directory to the path.\n",
    "\n",
    "Pytest searches the current directory and subdirectories for files that start with `test_` or end with `_test.py`. You can specify `testpaths` in the `pytest.ini` file to specify where to look for tests.\n",
    "\n",
    "Command line options:\n",
    "\n",
    "- `--doctest-modules` - run doctests in the module\n",
    "- `--doctest-glob=*.md` - run doctests in markdown files\n",
    "- `--pdb` - drop into the debugger on test failure\n",
    "- `-v` - verbose output (show NODEID)\n",
    "- `-q` - quiet output\n",
    "- `-m EXPR` - run tests with marks that match the expression\n",
    "- `-k EXPRESSION` - run tests with names that match the expression\n",
    "- `NODEID` - run a specific test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Tests\n",
    "\n",
    "By default, pytest will hide the output of a test that passes. You can use the `-s` option to show the output of a passing test. \n",
    "\n",
    "You can use the `--pdb` option to drop into the debugger on a test failure. \n",
    "\n",
    "Other options:\n",
    "\n",
    "- `-l` - show local variables\n",
    "- `--lf` - run the last failed test\n",
    "- `--maxfail=2` - stop after 2 failures\n",
    "- `-v` - show NODEID of tests\n",
    "- `-x` - stop after the first failure (`--maxfail=1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Careful with Output\n",
    "\n",
    "You don't want to print `-l` in CI if you have sensitive information in your tests. (Like secret keys.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint\n",
    "\n",
    "Consider combining `-x` and `--lf` to stop after the first failure and rerun the last failed test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctests\n",
    "\n",
    "Python has a built-in doctest module that can be used to test code in docstrings. Any code in a docstring that starts with `>>>` will be run and the output will be compared to the following lines.\n",
    "\n",
    "Here is a function with a simple doctest:\n",
    "\n",
    "```python\n",
    "def add(a, b):\n",
    "    \"\"\"\n",
    "    Add two numbers together.\n",
    "\n",
    "    >>> add(1, 2)\n",
    "    3\n",
    "    >>> add(3, 4)\n",
    "    7\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```\n",
    "\n",
    "You can run the doctests with `python -m doctest -v file.py` or `pytest --doctest-modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                           [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m______________________________ [doctest] __main__.code_with_bad_docs _______________________________\u001b[0m\n",
      "013 \n",
      "014     Add two numbers together\n",
      "015     \n",
      "016     >>> add(1, 2)\n",
      "Expected:\n",
      "    1\n",
      "Got:\n",
      "    3\n",
      "\n",
      "\u001b[1m\u001b[31mt_0fd94c1a4d2a48488d17128f29d4c46f.py\u001b[0m:16: DocTestFailure\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1m__main__.code_with_bad_docs\u001b[0m\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest --doctest-modules\n",
    "\n",
    "def add(a, b):\n",
    "    \"\"\"\n",
    "    Add two numbers together\n",
    "    \n",
    "    >>> add(1, 2)\n",
    "    3\n",
    "    >>> add(2, 3)\n",
    "    5\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def code_with_bad_docs(x,y):\n",
    "    \"\"\"\n",
    "    Add two numbers together\n",
    "    \n",
    "    >>> add(1, 2)\n",
    "    1\n",
    "    >>> add(2, 3)\n",
    "    5\n",
    "    \"\"\"\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures in Doctest\n",
    "\n",
    "If you have a fixture defined in `conftest.py` you can use it in a doctest with the `getfixture` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile conftest.py\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.fixture\n",
    "def sales():\n",
    "    return pd.DataFrame({'sales': [1, 2, 3, 1, 2, 3],\n",
    "                        'date': pd.date_range('2020-01-01', periods=6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_sales.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_sales.py\n",
    "\n",
    "def agg_sales(df):\n",
    "    \"\"\"\n",
    "    Aggregate sales data\n",
    "    >>> data = getfixture('sales') # needs to be a string\n",
    "    >>> agg_sales(data)\n",
    "                sales\n",
    "    date             \n",
    "    2020-01-01      1\n",
    "    2020-01-02      2\n",
    "    2020-01-03      3\n",
    "    2020-01-04      1\n",
    "    2020-01-05      2\n",
    "    2020-01-06      3\n",
    "    \"\"\"\n",
    "    return (df.groupby('date').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.14, pytest-7.2.0, pluggy-1.0.0 -- /Users/matt/.envs/menv/bin/python3.10\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/Users/matt/Dropbox/work/courses/ms-courses/professionalpython/03-Testing/.hypothesis/examples')\n",
      "rootdir: /Users/matt/Dropbox/work/courses/ms-courses/professionalpython/03-Testing\n",
      "plugins: dash-2.11.1, timeout-2.1.0, pytest_check_links-0.8.0, cov-4.0.0, hypothesis-6.81.2, console-scripts-1.3.1, anyio-3.6.2, typeguard-4.0.0, mock-3.14.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_sales.py::test_sales.agg_sales \u001b[32mPASSED\u001b[0m\u001b[32m                               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest --doctest-modules test_sales.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.14, pytest-7.2.0, pluggy-1.0.0 -- /Users/matt/.envs/menv/bin/python3.10\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/Users/matt/Dropbox/work/courses/ms-courses/professionalpython/03-Testing/.hypothesis/examples')\n",
      "rootdir: /Users/matt/Dropbox/work/courses/ms-courses/professionalpython/03-Testing\n",
      "plugins: dash-2.11.1, timeout-2.1.0, pytest_check_links-0.8.0, cov-4.0.0, hypothesis-6.81.2, console-scripts-1.3.1, anyio-3.6.2, typeguard-4.0.0, mock-3.14.0\n",
      "collected 0 items                                                              \u001b[0m\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.11s\u001b[0m\u001b[33m =============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_sales.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doctest Warts\n",
    "\n",
    "Doctests are whitespace sensitive. If you have a function that returns a string with a newline at the end, you need to include that newline in the doctest. If you have trailing whitespace in a doctest, it will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m______________________________ [doctest] __main__.trailing_whitespace ______________________________\u001b[0m\n",
      "004 \n",
      "005     Test that trailing whitespace is removed\n",
      "006     \n",
      "007     >>> print(trailing_whitespace())\n",
      "Expected:\n",
      "    no whitespace\n",
      "Got:\n",
      "    no whitespace \n",
      "\n",
      "\u001b[1m\u001b[31mt_0fd94c1a4d2a48488d17128f29d4c46f.py\u001b[0m:7: DocTestFailure\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1m__main__.trailing_whitespace\u001b[0m\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m2 deselected\u001b[0m\u001b[31m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest --doctest-modules -k trailing_whitespace\n",
    "\n",
    "import pytest\n",
    "\n",
    "def trailing_whitespace():\n",
    "    \"\"\"\n",
    "    Test that trailing whitespace is removed\n",
    "    \n",
    "    >>> print(trailing_whitespace())\n",
    "    no whitespace\n",
    "    \"\"\"\n",
    "    return 'no whitespace '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m____________________________________ [doctest] __main__.heading ____________________________________\u001b[0m\n",
      "005     Test that heading is added\n",
      "006 \n",
      "007     This works:\n",
      "008     >>> print(heading('GOOD'))\n",
      "009     <BLANKLINE>\n",
      "010     # GOOD\n",
      "011     <BLANKLINE>\n",
      "012     \n",
      "013     This does not:\n",
      "014     >>> print(heading('heading'))\n",
      "Expected nothing\n",
      "Got:\n",
      "    <BLANKLINE>\n",
      "    # heading\n",
      "    <BLANKLINE>\n",
      "\n",
      "\u001b[1m\u001b[31mt_0fd94c1a4d2a48488d17128f29d4c46f.py\u001b[0m:14: DocTestFailure\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1m__main__.heading\u001b[0m\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[31m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest --doctest-modules -k heading\n",
    "\n",
    "import pytest\n",
    "\n",
    "def heading(value):\n",
    "    \"\"\"\n",
    "    Test that heading is added\n",
    "\n",
    "    This works:\n",
    "    >>> print(heading('GOOD'))\n",
    "    <BLANKLINE>\n",
    "    # GOOD\n",
    "    <BLANKLINE>\n",
    "    \n",
    "    This does not:\n",
    "    >>> print(heading('heading'))\n",
    "\n",
    "    # heading\n",
    "\n",
    "    \"\"\"\n",
    "    return f\"\\n# {value}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Selection\n",
    "\n",
    "### Marking Tests\n",
    "\n",
    "You can *mark* tests with a decorator to give them attributes. You can then run tests based on these attributes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 2.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -k slow\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow():\n",
    "    time.sleep(1) # simulate a slow test\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow2():\n",
    "    time.sleep(1)\n",
    "    assert 2 == 2\n",
    "\n",
    "def test_normal():\n",
    "    assert 3 == 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m2 deselected\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -k \"not slow\"\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow():\n",
    "    time.sleep(1) # simulate a slow test\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow2():\n",
    "    time.sleep(1)\n",
    "    assert 2 == 2\n",
    "\n",
    "def test_normal():\n",
    "    assert 3 == 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 2.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -k \"time and not normal\"\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "# mark the whole module\n",
    "pytestmark = pytest.mark.time\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow():\n",
    "    time.sleep(1) # simulate a slow test\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow2():\n",
    "    time.sleep(1)\n",
    "    assert 2 == 2\n",
    "\n",
    "@pytest.mark.normal\n",
    "def test_normal():\n",
    "    assert 3 == 3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Marks\n",
    "\n",
    "Python is a language of typos. If you mistype a mark, pytest will not complain. You can register marks in a `pytest.ini` file to catch these typos.\n",
    "\n",
    "```ini\n",
    "[pytest]\n",
    "markers =\n",
    "    slow: mark a test as slow\n",
    "    fast: mark a test as fast\n",
    "```\n",
    "\n",
    "Run `pytest --markers` to see the registered marks.\n",
    "\n",
    "If you run `pytest --strict-markers` it will fail if you use an unregistered mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 2.04s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -k \"time and not normal\" --strict-markers\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "# mark the whole module\n",
    "pytestmark = pytest.mark.time\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_api():\n",
    "    time.sleep(1) # simulate a slow test\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_db():\n",
    "    time.sleep(1)\n",
    "    assert 2 == 2\n",
    "\n",
    "@pytest.mark.normal\n",
    "def test_local():\n",
    "    assert 3 == 3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Marks\n",
    "\n",
    "Pytest has some built-in marks:\n",
    "\n",
    "- `skip` - skip a test\n",
    "- `skipif` - skip a test if a condition is true\n",
    "- `xfail` - expect a test to fail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33mX\u001b[0m\u001b[33m                                                                                          [100%]\u001b[0m\n",
      "\u001b[33m\u001b[33m\u001b[1m2 skipped\u001b[0m, \u001b[33m\u001b[1m1 xpassed\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest \n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.skip(reason=\"no way of currently testing this\")\n",
    "def test_the_unknown():\n",
    "    assert 1 == 1\n",
    "\n",
    "\n",
    "@pytest.mark.skipif(os.environ.get('USER') == 'matt', \n",
    "                    reason='Matt is not allowed to run this test')\n",
    "def test_user():\n",
    "    assert 1 == 1\n",
    "\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_x():\n",
    "    assert ' 1'.lstrip() == '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Parametrization\n",
    "\n",
    "You can run the same test with different parameters using the `@pytest.mark.parametrize` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def parse_num_seq(txt):\n",
    "    \"\"\"\n",
    "    Parse a string of numbers separated by commas\n",
    "    \n",
    "    >>> parse_num_seq('1,2,3')\n",
    "    [1, 2, 3]\n",
    "    >>> parse_num_seq('1, 2, 3')\n",
    "    [1, 2, 3]\n",
    "    \"\"\"\n",
    "    return [int(x) for x in txt.split(',')] \n",
    "\n",
    "def test_parse_num_seq():\n",
    "    assert parse_num_seq('1,2,3') == [1, 2, 3]\n",
    "\n",
    "def test_parse_num_seq2():\n",
    "    assert parse_num_seq('1, 2, 4') == [1, 2, 4]\n",
    "\n",
    "def test_parse_num_seq3():\n",
    "    assert parse_num_seq('3,10, 20') == [3, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform darwin -- Python 3.10.14, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/matt/Dropbox/work/courses/ms-courses/professionalpython/03-Testing\n",
      "plugins: dash-2.11.1, timeout-2.1.0, pytest_check_links-0.8.0, cov-4.0.0, hypothesis-6.81.2, console-scripts-1.3.1, anyio-3.6.2, typeguard-4.0.0, mock-3.14.0\n",
      "collected 3 items\n",
      "\n",
      "t_0fd94c1a4d2a48488d17128f29d4c46f.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -v\n",
    "\n",
    "def parse_num_seq(txt):\n",
    "    \"\"\"\n",
    "    Parse a string of numbers separated by commas\n",
    "    \n",
    "    >>> parse_num_seq('1,2,3')\n",
    "    [1, 2, 3]\n",
    "    >>> parse_num_seq('1, 2, 3')\n",
    "    [1, 2, 3]\n",
    "    \"\"\"\n",
    "    return [int(x) for x in txt.split(',')] \n",
    "\n",
    "@pytest.mark.parametrize('txt, expected', [\n",
    "    ('1,2,3', [1, 2, 3]),\n",
    "    ('1, 2, 4', [1, 2, 4]),\n",
    "    ('3,10, 20', [3, 10, 20])\n",
    "])\n",
    "def test_parse_num_seq(txt, expected):\n",
    "    assert parse_num_seq(txt) == expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the NODEID in the output. This is the name of the test that was run.\n",
    "\n",
    "```\n",
    "% pytest -v\n",
    "==================================== test session starts ========================================================================\n",
    "platform darwin -- Python 3.10.14, pytest-7.2.0, pluggy-1.0.0 -- /Users/matt/.envs/menv/bin/python3.10\n",
    "cachedir: .pytest_cache\n",
    "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/private/tmp/foo/.hypothesis/examples')\n",
    "rootdir: /private/tmp/foo\n",
    "plugins: dash-2.11.1, timeout-2.1.0, pytest_check_links-0.8.0, cov-4.0.0, hypothesis-6.81.2, console-scripts-1.3.1, anyio-3.6.2, typeguard-4.0.0, mock-3.14.0\n",
    "collected 3 items\n",
    "\n",
    "test_parse.py::test_parse_num_seq[1,2,3-expected0] PASSED            [ 33%]\n",
    "test_parse.py::test_parse_num_seq[1, 2, 4-expected1] PASSED          [ 66%]\n",
    "test_parse.py::test_parse_num_seq[3,10, 20-expected2] PASSED         [100%]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures\n",
    "\n",
    "Fixtures are a way to set up and tear down resources for tests. They can be used to set up a database connection, create a temporary directory, or set up a model for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import pytest\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "@pytest.fixture\n",
    "def large_num():\n",
    "    return 1e20\n",
    "\n",
    "def test_large(large_num):\n",
    "    assert add(large_num, 1) == \\\n",
    "        large_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(1e20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "#method fixture\n",
    "\n",
    "def adder(a, b):\n",
    "    return a + b\n",
    "\n",
    "class TestAdder:\n",
    "\n",
    "    @pytest.fixture\n",
    "    def other_num(self):\n",
    "        return 42\n",
    "\n",
    "    def test_other(self, other_num):\n",
    "        assert adder(other_num, 1) == 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mcache\u001b[0m\u001b[33m -- .../_pytest/cacheprovider.py:510\u001b[0m\n",
      "    Return a cache object that can persist state between testing sessions.\n",
      "\n",
      "\u001b[32mcapsys\u001b[0m\u001b[33m -- .../_pytest/capture.py:905\u001b[0m\n",
      "    Enable text capturing of writes to ``sys.stdout`` and ``sys.stderr``.\n",
      "\n",
      "\u001b[32mcapsysbinary\u001b[0m\u001b[33m -- .../_pytest/capture.py:933\u001b[0m\n",
      "    Enable bytes capturing of writes to ``sys.stdout`` and ``sys.stderr``.\n",
      "\n",
      "\u001b[32mcapfd\u001b[0m\u001b[33m -- .../_pytest/capture.py:961\u001b[0m\n",
      "    Enable text capturing of writes to file descriptors ``1`` and ``2``.\n",
      "\n",
      "\u001b[32mcapfdbinary\u001b[0m\u001b[33m -- .../_pytest/capture.py:989\u001b[0m\n",
      "    Enable bytes capturing of writes to file descriptors ``1`` and ``2``.\n",
      "\n",
      "\u001b[32mdoctest_namespace\u001b[0m\u001b[36m [session scope]\u001b[0m\u001b[33m -- .../_pytest/doctest.py:738\u001b[0m\n",
      "    Fixture that returns a :py:class:`dict` that will be injected into the\n",
      "    namespace of doctests.\n",
      "\n",
      "\u001b[32mpytestconfig\u001b[0m\u001b[36m [session scope]\u001b[0m\u001b[33m -- .../_pytest/fixtures.py:1351\u001b[0m\n",
      "    Session-scoped fixture that returns the session's :class:`pytest.Config`\n",
      "    object.\n",
      "\n",
      "\u001b[32mrecord_property\u001b[0m\u001b[33m -- .../_pytest/junitxml.py:282\u001b[0m\n",
      "    Add extra properties to the calling test.\n",
      "\n",
      "\u001b[32mrecord_xml_attribute\u001b[0m\u001b[33m -- .../_pytest/junitxml.py:305\u001b[0m\n",
      "    Add extra xml attributes to the tag for the calling test.\n",
      "\n",
      "\u001b[32mrecord_testsuite_property\u001b[0m\u001b[36m [session scope]\u001b[0m\u001b[33m -- .../_pytest/junitxml.py:343\u001b[0m\n",
      "    Record a new ``<property>`` tag as child of the root ``<testsuite>``.\n",
      "\n",
      "\u001b[32mtmpdir_factory\u001b[0m\u001b[36m [session scope]\u001b[0m\u001b[33m -- .../_pytest/legacypath.py:302\u001b[0m\n",
      "    Return a :class:`pytest.TempdirFactory` instance for the test session.\n",
      "\n",
      "\u001b[32mtmpdir\u001b[0m\u001b[33m -- .../_pytest/legacypath.py:309\u001b[0m\n",
      "    Return a temporary directory path object which is unique to each test\n",
      "    function invocation, created as a sub directory of the base temporary\n",
      "    directory.\n",
      "\n",
      "\u001b[32mcaplog\u001b[0m\u001b[33m -- .../_pytest/logging.py:491\u001b[0m\n",
      "    Access and control log capturing.\n",
      "\n",
      "\u001b[32mmonkeypatch\u001b[0m\u001b[33m -- .../_pytest/monkeypatch.py:29\u001b[0m\n",
      "    A convenient fixture for monkey-patching.\n",
      "\n",
      "\u001b[32mrecwarn\u001b[0m\u001b[33m -- .../_pytest/recwarn.py:30\u001b[0m\n",
      "    Return a :class:`WarningsRecorder` instance that records all warnings emitted by test functions.\n",
      "\n",
      "\u001b[32mtmp_path_factory\u001b[0m\u001b[36m [session scope]\u001b[0m\u001b[33m -- .../_pytest/tmpdir.py:188\u001b[0m\n",
      "    Return a :class:`pytest.TempPathFactory` instance for the test session.\n",
      "\n",
      "\u001b[32mtmp_path\u001b[0m\u001b[33m -- .../_pytest/tmpdir.py:203\u001b[0m\n",
      "    Return a temporary directory path object which is unique to each test\n",
      "    function invocation, created as a sub directory of the base temporary\n",
      "    directory.\n",
      "\n",
      "\n",
      "---------------------------- fixtures defined from anyio.pytest_plugin -----------------------------\n",
      "\u001b[32manyio_backend\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/anyio/pytest_plugin.py:127\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32manyio_backend_name\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/anyio/pytest_plugin.py:132\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32manyio_backend_options\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/anyio/pytest_plugin.py:140\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\n",
      "---------------------------------- fixtures defined from conftest ----------------------------------\n",
      "\u001b[32msales\u001b[0m\u001b[33m -- conftest.py:6\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\n",
      "---------------------------- fixtures defined from dash.testing.plugin -----------------------------\n",
      "\u001b[32mdash_thread_server\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:130\u001b[0m\n",
      "    Start a local dash server in a new thread.\n",
      "\n",
      "\u001b[32mdash_process_server\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:137\u001b[0m\n",
      "    Start a Dash server with subprocess.Popen and waitress-serve.\n",
      "\n",
      "\u001b[32mdash_multi_process_server\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:144\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mdashr_server\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:150\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mdashjl_server\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:156\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mdash_br\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:162\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mdash_duo\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:178\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mdash_duo_mp\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:195\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mdashr\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:212\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mdashjl\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:229\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mdiskcache_manager\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/dash/testing/plugin.py:246\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\n",
      "--------------------------- fixtures defined from pytest_console_scripts ---------------------------\n",
      "\u001b[32mscript_launch_mode\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_console_scripts.py:277\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mscript_cwd\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_console_scripts.py:282\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mscript_runner\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_console_scripts.py:287\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\n",
      "----------------------------- fixtures defined from pytest_cov.plugin ------------------------------\n",
      "\u001b[32mno_cover\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_cov/plugin.py:393\u001b[0m\n",
      "    A pytest fixture to disable coverage.\n",
      "\n",
      "\u001b[32mcov\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_cov/plugin.py:399\u001b[0m\n",
      "    A pytest fixture to provide access to the underlying coverage object.\n",
      "\n",
      "\n",
      "----------------------------- fixtures defined from pytest_mock.plugin -----------------------------\n",
      "\u001b[32mclass_mocker\u001b[0m\u001b[36m [class scope]\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_mock/plugin.py:455\u001b[0m\n",
      "    Return an object that has the same interface to the `mock` module, but\n",
      "    takes care of automatically undoing all patches after each test method.\n",
      "\n",
      "\u001b[32mmocker\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_mock/plugin.py:455\u001b[0m\n",
      "    Return an object that has the same interface to the `mock` module, but\n",
      "    takes care of automatically undoing all patches after each test method.\n",
      "\n",
      "\u001b[32mmodule_mocker\u001b[0m\u001b[36m [module scope]\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_mock/plugin.py:455\u001b[0m\n",
      "    Return an object that has the same interface to the `mock` module, but\n",
      "    takes care of automatically undoing all patches after each test method.\n",
      "\n",
      "\u001b[32mpackage_mocker\u001b[0m\u001b[36m [package scope]\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_mock/plugin.py:455\u001b[0m\n",
      "    Return an object that has the same interface to the `mock` module, but\n",
      "    takes care of automatically undoing all patches after each test method.\n",
      "\n",
      "\u001b[32msession_mocker\u001b[0m\u001b[36m [session scope]\u001b[0m\u001b[33m -- ../../../../../../.envs/menv/lib/python3.10/site-packages/pytest_mock/plugin.py:455\u001b[0m\n",
      "    Return an object that has the same interface to the `mock` module, but\n",
      "    takes care of automatically undoing all patches after each test method.\n",
      "\n",
      "\n",
      "---------------------------------- fixtures defined from __main__ ----------------------------------\n",
      "\u001b[32mfail_fixture\u001b[0m\u001b[33m -- ../../../../../../../../var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/3662948318.py:11\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32mlarge_num\u001b[0m\u001b[33m -- ../../../../../../../../var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/3848286323.py:7\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\u001b[32msales\u001b[0m\u001b[33m -- ../../../../../../../../var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/4258747586.py:5\u001b[0m\n",
      "\u001b[31m    no docstring available\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[33m\u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest --fixtures\n",
    "\n",
    "# show out of the box fixtures and installed fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixture Tear Down\n",
    "\n",
    "There are a few ways to tear down a fixture:\n",
    "\n",
    "- `yield` - yield the fixture and run the teardown code after the test\n",
    "- `addfinalizer` - add a finalizer to the fixture\n",
    "- Use `setup` and `teardown` methods (`setup_module`/`setup_function`/`setup_class`/`setup_method`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stick some data in parquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "df.to_parquet('test.parquet')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def duckdb_con():\n",
    "    con = duckdb.connect()\n",
    "    yield con\n",
    "    con.close()\n",
    "\n",
    "def test_query(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "# use addfinalizer to close the connection\n",
    "@pytest.fixture\n",
    "def duckdb_con(request):\n",
    "    con = duckdb.connect()\n",
    "    request.addfinalizer(con.close)\n",
    "    return con\n",
    "\n",
    "def test_query(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "con = None\n",
    "\n",
    "def setup_module():\n",
    "    # called once for the module\n",
    "    global con\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "\n",
    "def teardown_module():\n",
    "    con.close()\n",
    "\n",
    "def test_query():\n",
    "    result = con.execute('SELECT * FROM test.parquet')\n",
    "    assert result.fetchone() == (1,4)\n",
    "\n",
    "def test_query2():\n",
    "    result = con.execute('SELECT sum(a) FROM test.parquet')\n",
    "    assert result.fetchone() == (6,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixture Scope\n",
    "\n",
    "Fixtures can have different scopes:\n",
    "\n",
    "- `function` - run once per test (default)\n",
    "- `class` - run once per class\n",
    "- `module` - run once per module\n",
    "- `session` - run once per session\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def duckdb_con():\n",
    "    con = duckdb.connect()\n",
    "    yield con\n",
    "    con.close()\n",
    "\n",
    "def test_query(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mE\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================== ERRORS ==============================================\n",
      "\u001b[31m\u001b[1m_____________________________________ ERROR at setup of test4 ______________________________________\u001b[0m\n",
      "ScopeMismatch: You tried to access the function scoped fixture two \u001b[94mwith\u001b[39;49;00m a session scoped request \u001b[96mobject\u001b[39;49;00m, involved factories:\u001b[90m\u001b[39;49;00m\n",
      "/var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/\u001b[94m3037045133.\u001b[39;49;00mpy:\u001b[94m6\u001b[39;49;00m:  \u001b[94mdef\u001b[39;49;00m \u001b[92mfour\u001b[39;49;00m(two)\u001b[90m\u001b[39;49;00m\n",
      "/var/folders/qn/r8_0pgj1645dn1w69vqls6cw0000gn/T/ipykernel_1036/\u001b[94m3037045133.\u001b[39;49;00mpy:\u001b[94m2\u001b[39;49;00m:  \u001b[94mdef\u001b[39;49;00m \u001b[92mtwo\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m t_0fd94c1a4d2a48488d17128f29d4c46f.py::\u001b[1mtest4\u001b[0m\n",
      "\u001b[31m\u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "# bad fixture depend\n",
    "@pytest.fixture(scope='function')\n",
    "def two():\n",
    "    return 2\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def four(two):\n",
    "    return two * two\n",
    "\n",
    "def test4(four):\n",
    "    assert four == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "# trigger skip from fixture\n",
    "import os\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def duckdb_con():\n",
    "    if not os.path.exists('test.parquet'):\n",
    "        pytest.skip('no test database')\n",
    "    con = duckdb.connect()\n",
    "    yield con\n",
    "    con.close()\n",
    "\n",
    "def test_query(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "# pass data from marks to fixture\n",
    "\n",
    "import os\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def duckdb_con(request):\n",
    "    # doesn't work if scope is session or module\n",
    "    mark = request.node.get_closest_marker('dbfile')\n",
    "    if mark is not None:\n",
    "        name = mark.args[0]\n",
    "    else:\n",
    "        name = None\n",
    "    if name is None or not os.path.exists(name):\n",
    "        pytest.skip('no test database')\n",
    "    con = duckdb.connect()        \n",
    "    return con\n",
    "\n",
    "@pytest.mark.dbfile('test.parquet')\n",
    "def test_query(duckdb_con, request):\n",
    "    db_name = request.node.get_closest_marker('dbfile').args[0]\n",
    "    df = duckdb_con.execute(f'SELECT * FROM {db_name}').fetchdf()\n",
    "    assert df.shape == (3, 2)\n",
    "\n",
    "@pytest.mark.dbfile('test.csv')\n",
    "def test_query2(duckdb_con):\n",
    "    db_name = request.node.get_closest_marker('dbfile').args[0]\n",
    "    df = duckdb_con.execute(f'SELECT * FROM {db_name}').fetchdf()\n",
    "    assert df.shape == (3, 2)\n",
    "\n",
    "def test_query3(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monkeypatch\n",
    "\n",
    "You can use the `monkeypatch` fixture to change the behavior of a function. This is useful for testing functions that call external services or functions that have side effects.\n",
    "\n",
    "I prefer to use this instead of mocking because I find it easier to understand what is happening.\n",
    "\n",
    "\n",
    "\n",
    "- *`monkeypatch.setattr()`*: Replaces functions or class methods with custom versions (e.g., lambdas), useful for mocking behaviors and testing edge cases.\n",
    "- *`monkeypatch.setenv()`*: Modifies environment variables, making it easy to configure test settings that depend on external environments.\n",
    "- *`monkeypatch.delattr()`*: Removes attributes temporarily, ideal for testing scenarios where attributes are absent.\n",
    "- *`monkeypatch.delenv()`*: Deletes environment variables temporarily for testing purposes.\n",
    "- All changes made with `monkeypatch` are temporary and only apply for the duration of the test, ensuring no side effects on other tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "import math\n",
    "\n",
    "def test_sin(monkeypatch):\n",
    "    monkeypatch.setattr(math, 'sin', lambda x: 42)\n",
    "    assert math.sin(0) == 42\n",
    "\n",
    "def test_sin_normal():\n",
    "    assert math.sin(0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Configuration\n",
    "\n",
    "- Rootdir\n",
    "\n",
    "  - Nodeid determined by the rootdir\n",
    "  - Plugins may store data in the rootdir\n",
    "  - Normally the rootdir is the directory where you run `pytest`\n",
    "\n",
    "Can put configuration in `pytest.ini` or in `pyproject.toml` (as of pytest 6.0)\n",
    "\n",
    "Common configuration options:\n",
    "\n",
    "- `minversion` - minimum version of pytest\n",
    "- `addopts = -v --strict-markers` - additional command line options\n",
    "- `testpaths` - directories to search for tests\n",
    "- `markers` - marks to register\n",
    "\n",
    "Example `pyproject.toml`:\n",
    "\n",
    "```toml\n",
    "[tool.pytest.ini_options]\n",
    "minversion = \"6.0\"\n",
    "addopts = \"-v --strict-markers\"\n",
    "testpaths = [\"tests\"]\n",
    "markers = [\n",
    "    \"slow: mark a test as slow\",\n",
    "    \"fast: mark a test as fast\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `conftest.py`\n",
    "\n",
    "`conftest.py` is a file that pytest looks for in the current directory and all parent directories. It can be used to define fixtures, marks, hooks, and plugins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Plugins\n",
    "\n",
    "Pytest has a rich plugin ecosystem. You can find plugins for:\n",
    "\n",
    "- pytest-cov - Code coverage\n",
    "- pytest-xdist - run tests in parallel\n",
    "- pytest-asyncio - asyncio support\n",
    "- pytest-timeout - add a timeout to tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
