{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Pytest\n",
    "\n",
    "- Easy test creation\n",
    "- Test runner\n",
    "- Test selection\n",
    "- Test parametrization\n",
    "- Test fixtures\n",
    "- Plugins\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "uv add --dev pytest\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Running tests\n",
    "\n",
    "```bash\n",
    "uv run pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Creation\n",
    "\n",
    "4 steps of a test:\n",
    "\n",
    "1. Setup\n",
    "2. Exercise\n",
    "3. Verify\n",
    "4. Teardown\n",
    "\n",
    "In pytest these steps are usually done with:\n",
    "\n",
    "1. Setup: Fixtures or setup methods\n",
    "2. Exercise: Call the function or method to be tested\n",
    "3. Verify: Use assert statements\n",
    "4. Teardown: Fixtures or teardown methods\n",
    "\n",
    "## Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_add():\n",
    "    assert add(1, 2) == 3\n",
    "    assert add(2, 3) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m_________________________________________ test_that_fails __________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_that_fails\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m add(\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m) == \u001b[94m3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m add(\u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == \u001b[94m6\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 5 == 6\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 5 = add(2, 3)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/cz/958tj1p10ys2ts9wc_77kgm00000gn/T/ipykernel_96255/2926093103.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_67895d22829c4a7b9305be28934014e0.py::\u001b[1mtest_that_fails\u001b[0m - assert 5 == 6\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_that_fails():\n",
    "    assert add(1, 2) == 3\n",
    "    assert add(2, 3) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Layout\n",
    "\n",
    "Tests are usually placed in a `tests` directory. Generally it does not need to be in the same directory as the code being tested. But the code that is being tested should be importable from the test directory. (This is why I like to do an editable install of the package I am testing.)\n",
    "\n",
    "add `tests/test_basic.py`:\n",
    "\n",
    "```python\n",
    "import sk_stepwise as sw\n",
    "\n",
    "\n",
    "def test_initialization():\n",
    "    model = None\n",
    "    rounds = []\n",
    "    optimizer = sw.StepwiseHyperoptOptimizer(model, rounds)\n",
    "    assert optimizer is not None\n",
    "```\n",
    "\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "uv run pytest\n",
    "```\n",
    "\n",
    "The output should be:\n",
    "\n",
    "```bash\n",
    "% uv run pytest\n",
    "===================== test session starts ===============================================\n",
    "platform darwin -- Python 3.12.5, pytest-8.3.3, pluggy-1.5.0\n",
    "rootdir: /private/tmp/sk-stepwise\n",
    "configfile: pyproject.toml\n",
    "collected 1 item\n",
    "\n",
    "tests/test_basic.py .                                                                                      [100%]\n",
    "\n",
    "================= warnings summary ================================================\n",
    ".venv/lib/python3.12/site-packages/hyperopt/atpe.py:19\n",
    "  /private/tmp/sk-stepwise/.venv/lib/python3.12/site-packages/hyperopt/atpe.py:19: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
    "    import pkg_resources\n",
    "\n",
    "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
    "============= 1 passed, 1 warning in 0.66s ==========================================\n",
    "matt@Matts-MacBook-Pro-4 sk-stepwise % cat tests/test_basic.py\n",
    "```\n",
    "\n",
    "## Test Output\n",
    "\n",
    "- `.` - test passed\n",
    "- `F` - test failed\n",
    "- `E` - test had an exception during fixture setup or teardown\n",
    "- `s` - test was skipped\n",
    "- `x` - expected failure\n",
    "- `X` - unexpected success (should have failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "IPython magic to first execute the cell, then execute [`ipytest.run()`][ipytest.run].\n",
       "\n",
       "**Note:** the magics are only available after running\n",
       "[`ipytest.autoconfig()`][ipytest.autoconfig] or\n",
       "[`ipytest.config(magics=True)`][ipytest.config].\n",
       "\n",
       "It cleans any previously found tests, i.e., only tests defined in the\n",
       "current cell are executed. To disable this behavior, use\n",
       "[`ipytest.config(clean=False)`][ipytest.config].\n",
       "\n",
       "Any arguments passed on the magic line are interpreted as command line\n",
       "arguments to to pytest. For example calling the magic as\n",
       "\n",
       "```python\n",
       "%%ipytest -qq\n",
       "```\n",
       "\n",
       "is equivalent to passing `-qq` to pytest. The arguments are formatted using\n",
       "Python's standard string formatting. Currently, only the `{MODULE}` variable\n",
       "is understood. It is replaced with the filename associated with the\n",
       "notebook. In addition node ids for tests can be generated by using the test\n",
       "name as a key, e.g., `{test_example}` will expand to\n",
       "`{MODULE}::test_example`.\n",
       "\n",
       "The keyword arguments passed to [`ipytest.run()`][ipytest.run] can be\n",
       "customized by including a comment of the form `# ipytest: arg1=value1,\n",
       "arg=value2` in the cell source. For example:\n",
       "\n",
       "```python\n",
       "%%ipytest {MODULE}::test1\n",
       "# ipytest: defopts=False\n",
       "```\n",
       "\n",
       "is equivalent to `ipytest.run(\"{MODULE}::test1\", defopts=False)`. In this\n",
       "case, it deactivates default arguments and then instructs pytest to only\n",
       "execute `test1`.\n",
       "\n",
       "**NOTE:** In the default configuration `%%ipytest` will not raise\n",
       "exceptions, when tests fail. To raise exceptions on test errors, e.g.,\n",
       "inside a CI/CD context, use `ipytest.autoconfig(raise_on_error=True)`.\n",
       "\u001b[0;31mFile:\u001b[0m      /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ipytest/_impl.py"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%ipytest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mE\u001b[0m\u001b[33ms\u001b[0m\u001b[33mx\u001b[0m\u001b[33mX\u001b[0m\u001b[31m                                                                                        [100%]\u001b[0m\n",
      "============================================== ERRORS ==============================================\n",
      "\u001b[31m\u001b[1m_____________________________________ ERROR at setup of test_E _____________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.fixture\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfail_fixture\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/cz/958tj1p10ys2ts9wc_77kgm00000gn/T/ipykernel_96255/3662948318.py\u001b[0m:12: NotImplementedError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m t_67895d22829c4a7b9305be28934014e0.py::\u001b[1mtest_E\u001b[0m - NotImplementedError\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "# the -qq is for quiet mode, which suppresses the output of the tests\n",
    " \n",
    "import os\n",
    "import pytest\n",
    "\n",
    "\n",
    "def test_period():\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.fixture\n",
    "def fail_fixture():\n",
    "    raise NotImplementedError\n",
    "\n",
    "def test_E(fail_fixture):\n",
    "    assert 1 == 1\n",
    "\n",
    "def test_skip():\n",
    "    if os.name == 'posix':\n",
    "        pytest.skip('skipping this test on posix')\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_x():\n",
    "    # assuming 3.14 adds the method .fancy_split() to the str class\n",
    "    assert '1,2-4'.fancy_split() == ['1', '2', '3', '4']\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_X():\n",
    "    assert ' 1'.lstrip() == '1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Different Outputs\n",
    "\n",
    "```python\n",
    "import sk_stepwise as sw\n",
    "import pytest\n",
    "\n",
    "\n",
    "def test_initialization():\n",
    "    model = None\n",
    "    rounds = []\n",
    "    optimizer = sw.StepwiseHyperoptOptimizer(model, rounds)\n",
    "    assert optimizer is not None\n",
    "\n",
    "def test_that_fails():\n",
    "    assert 'matt' == 'fred'\n",
    "\n",
    "@pytest.fixture\n",
    "def one():\n",
    "    return 1/0\n",
    "\n",
    "def test_with_exception(one):\n",
    "    assert one == 1\n",
    "\n",
    "@pytest.mark.xfail(raises=TypeError)\n",
    "def test_logistic():\n",
    "    from sklearn import linear_model\n",
    "    model = linear_model.LinearRegression()\n",
    "    rounds = []\n",
    "    opt = sw.StepwiseHyperoptOptimizer(model, rounds)\n",
    "    X = [[0,1], [0,2]]\n",
    "    y = [1, 0]\n",
    "    opt.fit(X, y)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The assert Statement\n",
    "\n",
    "Pytest rewrote the assert statement to give more information when it fails. It works with different types to give relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform darwin -- Python 3.12.6, pytest-8.2.0, pluggy-1.5.0 -- /Library/Frameworks/Python.framework/Versions/3.12/bin/python3.12\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/oscarpagani/projects/professional-python\n",
      "plugins: mimesis-16.0.0, anyio-4.3.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items\n",
      "\n",
      "t_67895d22829c4a7b9305be28934014e0.py::test_number \u001b[32mPASSED\u001b[0m\u001b[32m                                    [ 20%]\u001b[0m\n",
      "t_67895d22829c4a7b9305be28934014e0.py::test_string \u001b[31mFAILED\u001b[0m\u001b[31m                                    [ 40%]\u001b[0m\n",
      "t_67895d22829c4a7b9305be28934014e0.py::test_string_in \u001b[31mFAILED\u001b[0m\u001b[31m                                 [ 60%]\u001b[0m\n",
      "t_67895d22829c4a7b9305be28934014e0.py::test_list_in \u001b[31mFAILED\u001b[0m\u001b[31m                                   [ 80%]\u001b[0m\n",
      "t_67895d22829c4a7b9305be28934014e0.py::test_raise \u001b[32mPASSED\u001b[0m\u001b[31m                                     [100%]\u001b[0m\n",
      "\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m___________________________________________ test_string ____________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_string\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mThe cold brown fox ate a bird\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33mThe jumping brown fox ate a bird\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'The cold bro...ox ate a bird' == 'The jumping ...ox ate a bird'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         - The jumping brown fox ate a bird\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?     ^^^^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + The cold brown fox ate a bird\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?     ^^^^\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/cz/958tj1p10ys2ts9wc_77kgm00000gn/T/ipykernel_96255/1061909615.py\u001b[0m:7: AssertionError\n",
      "\u001b[31m\u001b[1m__________________________________________ test_string_in __________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_string_in\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[95min\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mmatt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'b' in 'matt'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/cz/958tj1p10ys2ts9wc_77kgm00000gn/T/ipykernel_96255/1061909615.py\u001b[0m:10: AssertionError\n",
      "\u001b[31m\u001b[1m___________________________________________ test_list_in ___________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_list_in\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94m5_000\u001b[39;49;00m \u001b[95min\u001b[39;49;00m \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[94m1000\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 5000 in [0, 1, 2, 3, 4, 5, ...]\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where [0, 1, 2, 3, 4, 5, ...] = list(range(0, 1000))\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where range(0, 1000) = range(1000)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/cz/958tj1p10ys2ts9wc_77kgm00000gn/T/ipykernel_96255/1061909615.py\u001b[0m:13: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_67895d22829c4a7b9305be28934014e0.py::\u001b[1mtest_string\u001b[0m - AssertionError: assert 'The cold bro...ox ate a bird' == 'The jumping ...ox ate a bird'\n",
      "\u001b[31mFAILED\u001b[0m t_67895d22829c4a7b9305be28934014e0.py::\u001b[1mtest_string_in\u001b[0m - AssertionError: assert 'b' in 'matt'\n",
      "\u001b[31mFAILED\u001b[0m t_67895d22829c4a7b9305be28934014e0.py::\u001b[1mtest_list_in\u001b[0m - assert 5000 in [0, 1, 2, 3, 4, 5, ...]\n",
      "\u001b[31m=================================== \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.35s\u001b[0m\u001b[31m ====================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_number():\n",
    "    assert 1 == 1\n",
    "\n",
    "def test_string():\n",
    "    assert 'The cold brown fox ate a bird' == 'The jumping brown fox ate a bird'\n",
    "\n",
    "def test_string_in():\n",
    "    assert 'b' in 'matt'\n",
    "\n",
    "def test_list_in():\n",
    "    assert 5_000 in list(range(1000))\n",
    "\n",
    "def test_raise():\n",
    "    # no assertion in this one\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        1 / 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `pytest.raises` not found.\n"
     ]
    }
   ],
   "source": [
    "pytest.raises??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mpython_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRaisesContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexpected_exception\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmatch_expr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPattern\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      An abstract base class for context managers.\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mRaisesContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mContextManager\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExceptionInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexpected_exception\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmatch_expr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPattern\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_exception\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_expr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_expr\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcinfo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExceptionInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExceptionInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExceptionInfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_later\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcinfo\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseException\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexc_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseException\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexc_tb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTracebackType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mexc_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcinfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Cast to narrow the exception type now that it's verified.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracebackType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_unfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_expr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_expr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/_pytest/python_api.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "python_api.RaisesContext??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m___________________________________________ test_number ____________________________________________\u001b[0m\n",
      "\n",
      "sales =    sales       date\n",
      "0      1 2020-01-01\n",
      "1      2 2020-01-02\n",
      "2      3 2020-01-03\n",
      "3      1 2020-01-04\n",
      "4      2 2020-01-05\n",
      "5      3 2020-01-06\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_number\u001b[39;49;00m(sales):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(sales.index, pd.DatetimeIndex), \u001b[33m'\u001b[39;49;00m\u001b[33mShould be a timeseries index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Should be a timeseries index\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = isinstance(RangeIndex(start=0, stop=6, step=1), <class 'pandas.core.indexes.datetimes.DatetimeIndex'>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where RangeIndex(start=0, stop=6, step=1) =    sales       date\\n0      1 2020-01-01\\n1      2 2020-01-02\\n2      3 2020-01-03\\n3      1 2020-01-04\\n4      2 2020-01-05\\n5      3 2020-01-06.index\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   <class 'pandas.core.indexes.datetimes.DatetimeIndex'> = pd.DatetimeIndex\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/cz/958tj1p10ys2ts9wc_77kgm00000gn/T/ipykernel_96255/4258747586.py\u001b[0m:10: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_67895d22829c4a7b9305be28934014e0.py::\u001b[1mtest_number\u001b[0m - AssertionError: Should be a timeseries index\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "# Can provide addtional text upon failure\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.fixture\n",
    "def sales():\n",
    "    return pd.DataFrame({'sales': [1, 2, 3, 1, 2, 3],\n",
    "                            'date': pd.date_range('2020-01-01', periods=6)})\n",
    "\n",
    "def test_number(sales):\n",
    "    assert isinstance(sales.index, pd.DatetimeIndex), 'Should be a timeseries index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Runner\n",
    "\n",
    "`pytest` assumes that the code you are testing is installed. It does not add the current directory to the path. So you need to install the package you are testing. You can do this with `pip install -e .` or `uv run pip install -e .`. If you run `python -m pytest` it will add the current directory to the path.\n",
    "\n",
    "Pytest searches the current directory and subdirectories for files that start with `test_` or end with `_test.py`. You can specify `testpaths` in the `pytest.ini` file to specify where to look for tests.\n",
    "\n",
    "Command line options:\n",
    "\n",
    "- `--doctest-modules` - run doctests in the module\n",
    "- `--doctest-glob=*.md` - run doctests in markdown files\n",
    "- `--pdb` - drop into the debugger on test failure\n",
    "- `-v` - verbose output (show NODEID)\n",
    "- `-q` - quiet output\n",
    "- `-m EXPR` - run tests with marks that match the expression\n",
    "- `-k EXPRESSION` - run tests with names that match the expression\n",
    "- `NODEID` - run a specific test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Tests\n",
    "\n",
    "By default, pytest will hide the output of a test that passes. You can use the `-s` option to show the output of a passing test. \n",
    "\n",
    "You can use the `--pdb` option to drop into the debugger on a test failure. \n",
    "\n",
    "Other options:\n",
    "\n",
    "- `-l` - show local variables\n",
    "- `--lf` - run the last failed test\n",
    "- `--maxfail=2` - stop after 2 failures\n",
    "- `-v` - show NODEID of tests\n",
    "- `-x` - stop after the first failure (`--maxfail=1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Careful with Output\n",
    "\n",
    "You don't want to print `-l` in CI if you have sensitive information in your tests. (Like secret keys.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint\n",
    "\n",
    "Consider combining `-x` and `--lf` to stop after the first failure and rerun the last failed test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctests\n",
    "\n",
    "Python has a built-in doctest module that can be used to test code in docstrings. Any code in a docstring that starts with `>>>` will be run and the output will be compared to the following lines.\n",
    "\n",
    "Here is a function with a simple doctest:\n",
    "\n",
    "```python\n",
    "def add(a, b):\n",
    "    \"\"\"\n",
    "    Add two numbers together.\n",
    "\n",
    "    >>> add(1, 2)\n",
    "    3\n",
    "    >>> add(3, 4)\n",
    "    7\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```\n",
    "\n",
    "You can run the doctests with `python -m doctest -v file.py` or `pytest --doctest-modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                           [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m______________________________ [doctest] __main__.code_with_bad_docs _______________________________\u001b[0m\n",
      "013 \n",
      "014     Add two numbers together\n",
      "015     \n",
      "016     >>> add(1, 2)\n",
      "Expected:\n",
      "    1\n",
      "Got:\n",
      "    3\n",
      "\n",
      "\u001b[1m\u001b[31mt_67895d22829c4a7b9305be28934014e0.py\u001b[0m:16: DocTestFailure\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_67895d22829c4a7b9305be28934014e0.py::\u001b[1m__main__.code_with_bad_docs\u001b[0m\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest --doctest-modules\n",
    "\n",
    "def add(a, b):\n",
    "    \"\"\"\n",
    "    Add two numbers together\n",
    "    \n",
    "    >>> add(1, 2)\n",
    "    3\n",
    "    >>> add(2, 3)\n",
    "    5\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def code_with_bad_docs(x,y):\n",
    "    \"\"\"\n",
    "    Add two numbers together\n",
    "    \n",
    "    >>> add(1, 2)\n",
    "    1\n",
    "    >>> add(2, 3)\n",
    "    5\n",
    "    \"\"\"\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures in Doctest\n",
    "\n",
    "If you have a fixture defined in `conftest.py` you can use it in a doctest with the `getfixture` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conftest.py\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.fixture\n",
    "def sales():\n",
    "    return pd.DataFrame({'sales': [1, 2, 3, 1, 2, 3],\n",
    "                        'date': pd.date_range('2020-01-01', periods=6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_sales.py\n",
    "\n",
    "def agg_sales(df):\n",
    "    \"\"\"\n",
    "    Aggregate sales data\n",
    "    >>> data = getfixture('sales') # needs to be a string\n",
    "    >>> agg_sales(data)\n",
    "                sales\n",
    "    date             \n",
    "    2020-01-01      1\n",
    "    2020-01-02      2\n",
    "    2020-01-03      3\n",
    "    2020-01-04      1\n",
    "    2020-01-05      2\n",
    "    2020-01-06      3\n",
    "    \"\"\"\n",
    "    return (df.groupby('date').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --doctest-modules test_sales.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_sales.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doctest Warts\n",
    "\n",
    "Doctests are whitespace sensitive. If you have a function that returns a string with a newline at the end, you need to include that newline in the doctest. If you have trailing whitespace in a doctest, it will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest --doctest-modules -k trailing_whitespace\n",
    "\n",
    "import pytest\n",
    "\n",
    "def trailing_whitespace():\n",
    "    \"\"\"\n",
    "    Test that trailing whitespace is removed\n",
    "    \n",
    "    >>> print(trailing_whitespace())\n",
    "    no whitespace\n",
    "    \"\"\"\n",
    "    return 'no whitespace '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest --doctest-modules -k heading\n",
    "\n",
    "import pytest\n",
    "\n",
    "def heading(value):\n",
    "    \"\"\"\n",
    "    Test that heading is added\n",
    "\n",
    "    This works:\n",
    "    >>> print(heading('GOOD'))\n",
    "    <BLANKLINE>\n",
    "    # GOOD\n",
    "    <BLANKLINE>\n",
    "    \n",
    "    This does not:\n",
    "    >>> print(heading('heading'))\n",
    "\n",
    "    # heading\n",
    "\n",
    "    \"\"\"\n",
    "    return f\"\\n# {value}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Selection\n",
    "\n",
    "### Marking Tests\n",
    "\n",
    "You can *mark* tests with a decorator to give them attributes. You can then run tests based on these attributes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -k slow\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow():\n",
    "    time.sleep(1) # simulate a slow test\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow2():\n",
    "    time.sleep(1)\n",
    "    assert 2 == 2\n",
    "\n",
    "def test_normal():\n",
    "    assert 3 == 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -k \"not slow\"\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow():\n",
    "    time.sleep(1) # simulate a slow test\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow2():\n",
    "    time.sleep(1)\n",
    "    assert 2 == 2\n",
    "\n",
    "def test_normal():\n",
    "    assert 3 == 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -k \"time and not normal\"\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "# mark the whole module\n",
    "pytestmark = pytest.mark.time\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow():\n",
    "    time.sleep(1) # simulate a slow test\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow2():\n",
    "    time.sleep(1)\n",
    "    assert 2 == 2\n",
    "\n",
    "@pytest.mark.normal\n",
    "def test_normal():\n",
    "    assert 3 == 3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Marks\n",
    "\n",
    "Python is a language of typos. If you mistype a mark, pytest will not complain. You can register marks in a `pytest.ini` file to catch these typos.\n",
    "\n",
    "```ini\n",
    "[pytest]\n",
    "markers =\n",
    "    slow: mark a test as slow\n",
    "    fast: mark a test as fast\n",
    "```\n",
    "\n",
    "Run `pytest --markers` to see the registered marks.\n",
    "\n",
    "If you run `pytest --strict-markers` it will fail if you use an unregistered mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -k \"time and not normal\" --strict-markers\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "# mark the whole module\n",
    "pytestmark = pytest.mark.time\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_api():\n",
    "    time.sleep(1) # simulate a slow test\n",
    "    assert 1 == 1\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_db():\n",
    "    time.sleep(1)\n",
    "    assert 2 == 2\n",
    "\n",
    "@pytest.mark.normal\n",
    "def test_local():\n",
    "    assert 3 == 3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Marks\n",
    "\n",
    "Pytest has some built-in marks:\n",
    "\n",
    "- `skip` - skip a test\n",
    "- `skipif` - skip a test if a condition is true\n",
    "- `xfail` - expect a test to fail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest \n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.skip(reason=\"no way of currently testing this\")\n",
    "def test_the_unknown():\n",
    "    assert 1 == 1\n",
    "\n",
    "\n",
    "@pytest.mark.skipif(os.environ.get('USER') == 'matt', \n",
    "                    reason='Matt is not allowed to run this test')\n",
    "def test_user():\n",
    "    assert 1 == 1\n",
    "\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_x():\n",
    "    assert ' 1'.lstrip() == '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Parametrization\n",
    "\n",
    "You can run the same test with different parameters using the `@pytest.mark.parametrize` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def parse_num_seq(txt):\n",
    "    \"\"\"\n",
    "    Parse a string of numbers separated by commas\n",
    "    \n",
    "    >>> parse_num_seq('1,2,3')\n",
    "    [1, 2, 3]\n",
    "    >>> parse_num_seq('1, 2, 3')\n",
    "    [1, 2, 3]\n",
    "    \"\"\"\n",
    "    return [int(x) for x in txt.split(',')] \n",
    "\n",
    "def test_parse_num_seq():\n",
    "    assert parse_num_seq('1,2,3') == [1, 2, 3]\n",
    "\n",
    "def test_parse_num_seq2():\n",
    "    assert parse_num_seq('1, 2, 4') == [1, 2, 4]\n",
    "\n",
    "def test_parse_num_seq3():\n",
    "    assert parse_num_seq('3,10, 20') == [3, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -v\n",
    "\n",
    "def parse_num_seq(txt):\n",
    "    \"\"\"\n",
    "    Parse a string of numbers separated by commas\n",
    "    \n",
    "    >>> parse_num_seq('1,2,3')\n",
    "    [1, 2, 3]\n",
    "    >>> parse_num_seq('1, 2, 3')\n",
    "    [1, 2, 3]\n",
    "    \"\"\"\n",
    "    return [int(x) for x in txt.split(',')] \n",
    "\n",
    "@pytest.mark.parametrize('txt, expected', [\n",
    "    ('1,2,3', [1, 2, 3]),\n",
    "    ('1, 2, 4', [1, 2, 4]),\n",
    "    ('3,10, 20', [3, 10, 20])\n",
    "])\n",
    "def test_parse_num_seq(txt, expected):\n",
    "    assert parse_num_seq(txt) == expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the NODEID in the output. This is the name of the test that was run.\n",
    "\n",
    "```\n",
    "% pytest -v\n",
    "==================================== test session starts ========================================================================\n",
    "platform darwin -- Python 3.10.14, pytest-7.2.0, pluggy-1.0.0 -- /Users/matt/.envs/menv/bin/python3.10\n",
    "cachedir: .pytest_cache\n",
    "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/private/tmp/foo/.hypothesis/examples')\n",
    "rootdir: /private/tmp/foo\n",
    "plugins: dash-2.11.1, timeout-2.1.0, pytest_check_links-0.8.0, cov-4.0.0, hypothesis-6.81.2, console-scripts-1.3.1, anyio-3.6.2, typeguard-4.0.0, mock-3.14.0\n",
    "collected 3 items\n",
    "\n",
    "test_parse.py::test_parse_num_seq[1,2,3-expected0] PASSED            [ 33%]\n",
    "test_parse.py::test_parse_num_seq[1, 2, 4-expected1] PASSED          [ 66%]\n",
    "test_parse.py::test_parse_num_seq[3,10, 20-expected2] PASSED         [100%]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures\n",
    "\n",
    "Fixtures are a way to set up and tear down resources for tests. They can be used to set up a database connection, create a temporary directory, or set up a model for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "import pytest\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "@pytest.fixture\n",
    "def large_num():\n",
    "    return 1e20\n",
    "\n",
    "def test_large(large_num):\n",
    "    assert add(large_num, 1) == \\\n",
    "        large_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(1e20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "#method fixture\n",
    "\n",
    "def adder(a, b):\n",
    "    return a + b\n",
    "\n",
    "class TestAdder:\n",
    "\n",
    "    @pytest.fixture\n",
    "    def other_num(self):\n",
    "        return 42\n",
    "\n",
    "    def test_other(self, other_num):\n",
    "        assert adder(other_num, 1) == 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest --fixtures\n",
    "\n",
    "# show out of the box fixtures and installed fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixture Tear Down\n",
    "\n",
    "There are a few ways to tear down a fixture:\n",
    "\n",
    "- `yield` - yield the fixture and run the teardown code after the test\n",
    "- `addfinalizer` - add a finalizer to the fixture\n",
    "- Use `setup` and `teardown` methods (`setup_module`/`setup_function`/`setup_class`/`setup_method`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stick some data in parquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n",
    "df.to_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def duckdb_con():\n",
    "    con = duckdb.connect()\n",
    "    yield con\n",
    "    con.close()\n",
    "\n",
    "def test_query(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "# use addfinalizer to close the connection\n",
    "@pytest.fixture\n",
    "def duckdb_con(request):\n",
    "    con = duckdb.connect()\n",
    "    request.addfinalizer(con.close)\n",
    "    return con\n",
    "\n",
    "def test_query(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "con = None\n",
    "\n",
    "def setup_module():\n",
    "    # called once for the module\n",
    "    global con\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "\n",
    "def teardown_module():\n",
    "    con.close()\n",
    "\n",
    "def test_query():\n",
    "    result = con.execute('SELECT * FROM test.parquet')\n",
    "    assert result.fetchone() == (1,4)\n",
    "\n",
    "def test_query2():\n",
    "    result = con.execute('SELECT sum(a) FROM test.parquet')\n",
    "    assert result.fetchone() == (6,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixture Scope\n",
    "\n",
    "Fixtures can have different scopes:\n",
    "\n",
    "- `function` - run once per test (default)\n",
    "- `class` - run once per class\n",
    "- `module` - run once per module\n",
    "- `session` - run once per session\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def duckdb_con():\n",
    "    con = duckdb.connect()\n",
    "    yield con\n",
    "    con.close()\n",
    "\n",
    "def test_query(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "# bad fixture depend\n",
    "@pytest.fixture(scope='function')\n",
    "def two():\n",
    "    return 2\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def four(two):\n",
    "    return two * two\n",
    "\n",
    "def test4(four):\n",
    "    assert four == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "# trigger skip from fixture\n",
    "import os\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def duckdb_con():\n",
    "    if not os.path.exists('test.parquet'):\n",
    "        pytest.skip('no test database')\n",
    "    con = duckdb.connect()\n",
    "    yield con\n",
    "    con.close()\n",
    "\n",
    "def test_query(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -s\n",
    "# pass data from marks to fixture\n",
    "\n",
    "import os\n",
    "import duckdb\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def duckdb_con(request):\n",
    "    # doesn't work if scope is session or module\n",
    "    mark = request.node.get_closest_marker('dbfile')\n",
    "    if mark is not None:\n",
    "        name = mark.args[0]\n",
    "    else:\n",
    "        name = None\n",
    "    if name is None or not os.path.exists(name):\n",
    "        pytest.skip('no test database')\n",
    "    con = duckdb.connect()        \n",
    "    return con\n",
    "\n",
    "@pytest.mark.dbfile('test.parquet')\n",
    "def test_query(duckdb_con, request):\n",
    "    db_name = request.node.get_closest_marker('dbfile').args[0]\n",
    "    df = duckdb_con.execute(f'SELECT * FROM {db_name}').fetchdf()\n",
    "    assert df.shape == (3, 2)\n",
    "\n",
    "@pytest.mark.dbfile('test.csv')\n",
    "def test_query2(duckdb_con):\n",
    "    db_name = request.node.get_closest_marker('dbfile').args[0]\n",
    "    df = duckdb_con.execute(f'SELECT * FROM {db_name}').fetchdf()\n",
    "    assert df.shape == (3, 2)\n",
    "\n",
    "def test_query3(duckdb_con):\n",
    "    df = duckdb_con.execute('SELECT * FROM test.parquet').fetchdf()\n",
    "    assert df.shape == (3, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monkeypatch\n",
    "\n",
    "You can use the `monkeypatch` fixture to change the behavior of a function. This is useful for testing functions that call external services or functions that have side effects.\n",
    "\n",
    "I prefer to use this instead of mocking because I find it easier to understand what is happening.\n",
    "\n",
    "\n",
    "\n",
    "- *`monkeypatch.setattr()`*: Replaces functions or class methods with custom versions (e.g., lambdas), useful for mocking behaviors and testing edge cases.\n",
    "- *`monkeypatch.setenv()`*: Modifies environment variables, making it easy to configure test settings that depend on external environments.\n",
    "- *`monkeypatch.delattr()`*: Removes attributes temporarily, ideal for testing scenarios where attributes are absent.\n",
    "- *`monkeypatch.delenv()`*: Deletes environment variables temporarily for testing purposes.\n",
    "- All changes made with `monkeypatch` are temporary and only apply for the duration of the test, ensuring no side effects on other tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "import math\n",
    "\n",
    "def test_sin(monkeypatch):\n",
    "    monkeypatch.setattr(math, 'sin', lambda x: 42)\n",
    "    assert math.sin(0) == 42\n",
    "\n",
    "def test_sin_normal():\n",
    "    assert math.sin(0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Configuration\n",
    "\n",
    "- Rootdir\n",
    "\n",
    "  - Nodeid determined by the rootdir\n",
    "  - Plugins may store data in the rootdir\n",
    "  - Normally the rootdir is the directory where you run `pytest`\n",
    "\n",
    "Can put configuration in `pytest.ini` or in `pyproject.toml` (as of pytest 6.0)\n",
    "\n",
    "Common configuration options:\n",
    "\n",
    "- `minversion` - minimum version of pytest\n",
    "- `addopts = -v --strict-markers` - additional command line options\n",
    "- `testpaths` - directories to search for tests\n",
    "- `markers` - marks to register\n",
    "\n",
    "Example `pyproject.toml`:\n",
    "\n",
    "```toml\n",
    "[tool.pytest.ini_options]\n",
    "minversion = \"6.0\"\n",
    "addopts = \"-v --strict-markers\"\n",
    "testpaths = [\"tests\"]\n",
    "markers = [\n",
    "    \"slow: mark a test as slow\",\n",
    "    \"fast: mark a test as fast\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `conftest.py`\n",
    "\n",
    "`conftest.py` is a file that pytest looks for in the current directory and all parent directories. It can be used to define fixtures, marks, hooks, and plugins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Plugins\n",
    "\n",
    "Pytest has a rich plugin ecosystem. You can find plugins for:\n",
    "\n",
    "- pytest-cov - Code coverage\n",
    "- pytest-xdist - run tests in parallel\n",
    "- pytest-asyncio - asyncio support\n",
    "- pytest-timeout - add a timeout to tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
